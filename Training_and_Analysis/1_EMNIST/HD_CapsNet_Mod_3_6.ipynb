{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b24df6d",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"6\"><center><b> HD-CapsNet: A Hierarchical Deep Capsule Network for Image Classification </b></center></font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691c83f",
   "metadata": {},
   "source": [
    "**Changing Model Architecture**\n",
    "- **(Mod-3.1)** 16-D > 8-D (Coarse > FINE) use skip connections between Secondary Capsules $Concatenate([P_{caps}, S_{coarse}])$ > input for $S_{medium}$ and $Concatenate([P_{caps}, S_{medium}])$ > input for $S_{fine}$\n",
    "- With $L_{Cons}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9d6d5",
   "metadata": {},
   "source": [
    "# Files and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4aa737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:04:39.664619Z",
     "iopub.status.busy": "2023-02-28T11:04:39.662620Z",
     "iopub.status.idle": "2023-02-28T11:04:52.368607Z",
     "shell.execute_reply": "2023-02-28T11:04:52.369606Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "# Supporting Libraries:\n",
    "    #Mathplot lib for ploting graphs\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "    # numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "    #system\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "    #import other libraries\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "from treelib import Tree\n",
    "    # ML model, Dataset and evalution metrics\n",
    "sys.path.append('../../') ### adding system parth for src folder\n",
    "from src import datasets # load datasets\n",
    "from src import MixUp # load datasets\n",
    "from src import MixUp_add_loss # load datasets\n",
    "from src import metrics # load hierarchcial metrics\n",
    "from src import sysenv # load hierarchcial metrics\n",
    "from src import models # load machine learning models\n",
    "\n",
    "    ## Tensorflow_docs\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "    # Auto reload local libraries if updated\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c610b",
   "metadata": {},
   "source": [
    "# System information & GPU growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c959d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:04:52.377607Z",
     "iopub.status.busy": "2023-02-28T11:04:52.376544Z",
     "iopub.status.idle": "2023-02-28T11:04:52.694552Z",
     "shell.execute_reply": "2023-02-28T11:04:52.695551Z"
    }
   },
   "outputs": [],
   "source": [
    "systeminfo = sysenv.systeminfo()\n",
    "print(systeminfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d5a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:04:52.705577Z",
     "iopub.status.busy": "2023-02-28T11:04:52.703575Z",
     "iopub.status.idle": "2023-02-28T11:04:54.020385Z",
     "shell.execute_reply": "2023-02-28T11:04:54.021384Z"
    }
   },
   "outputs": [],
   "source": [
    "gpus = \"0,1,2,3,4,5,6,7\" ## Selecting Available gpus (Multi-GPUS)\n",
    "gpus = \"0\" ## Selecting Available gpus (Single GPU)\n",
    "gpugrowth = sysenv.gpugrowth(gpus = gpus) ## Limiting GPUS from OS environment\n",
    "gpugrowth.memory_growth() #GPU memory growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd551837",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ee1d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:04:54.032317Z",
     "iopub.status.busy": "2023-02-28T11:04:54.030384Z",
     "iopub.status.idle": "2023-02-28T11:04:54.420357Z",
     "shell.execute_reply": "2023-02-28T11:04:54.420357Z"
    }
   },
   "outputs": [],
   "source": [
    "train_params = {\"n_epochs\" : 100,\n",
    "                \"batch_size\": 64,\n",
    "                \"lr\": 0.001, # Initial learning rate\n",
    "                \"lr_decay\": 0.95, # Learning rate decay\n",
    "                \"decay_exe\": 9, #learning rate decay execution epoch after\n",
    "               }\n",
    "model_params = {\"optimizer\": tf.keras.optimizers.Adam(train_params['lr']),\n",
    "                \"loss_function\": models.MarginLoss(),\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98815c",
   "metadata": {},
   "source": [
    "# log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ace371",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:04:54.427352Z",
     "iopub.status.busy": "2023-02-28T11:04:54.426353Z",
     "iopub.status.idle": "2023-02-28T11:04:54.762048Z",
     "shell.execute_reply": "2023-02-28T11:04:54.763023Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory = sysenv.log_dir('1_EMNIST/HD_CapsNet/Mod_3_6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfde972",
   "metadata": {},
   "source": [
    "# Import Dataset : FMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714d33f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:04:54.767047Z",
     "iopub.status.busy": "2023-02-28T11:04:54.767047Z",
     "iopub.status.idle": "2023-02-28T11:04:56.771530Z",
     "shell.execute_reply": "2023-02-28T11:04:56.772530Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = datasets.E_MNIST(version = 'ALL') # importing Dataset\n",
    "# dataset = datasets.E_MNIST(version = 'reduce') # importing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38ee07",
   "metadata": {},
   "source": [
    "## Learning Rate Decay Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed77d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:04:56.780533Z",
     "iopub.status.busy": "2023-02-28T11:04:56.780533Z",
     "iopub.status.idle": "2023-02-28T11:04:57.145726Z",
     "shell.execute_reply": "2023-02-28T11:04:57.145726Z"
    }
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    learning_rate_init = train_params[\"lr\"]\n",
    "    \n",
    "    if epoch > train_params[\"decay_exe\"]:\n",
    "        learning_rate_init = train_params[\"lr\"] * (train_params[\"lr_decay\"] ** (epoch-9))\n",
    "        \n",
    "    tf.summary.scalar('learning rate', data=learning_rate_init, step=epoch)\n",
    "        \n",
    "    return learning_rate_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5ea00",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee6349a",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a7b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:04:57.154726Z",
     "iopub.status.busy": "2023-02-28T11:04:57.154726Z",
     "iopub.status.idle": "2023-02-28T11:04:57.655791Z",
     "shell.execute_reply": "2023-02-28T11:04:57.654725Z"
    }
   },
   "outputs": [],
   "source": [
    "number_of_classes_c = len(np.unique(np.argmax(dataset['y_train_coarse'], axis=1)))\n",
    "number_of_classes_f = len(np.unique(np.argmax(dataset['y_train_fine'], axis=1)))\n",
    "\n",
    "## For Dynamic LossWeights\n",
    "initial_lw = models.initial_lw({\"coarse\": number_of_classes_c,\n",
    "                                \"fine\": number_of_classes_f})\n",
    "\n",
    "lossweight = {'coarse_lw' : K.variable(value = initial_lw['coarse'], dtype=\"float32\", name=\"coarse_lw\"),\n",
    "             'fine_lw' : K.variable(value = initial_lw['fine'], dtype=\"float32\", name=\"fine_lw\"),\n",
    "              'consistency_lw' : 0.1\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a92ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:04:57.665726Z",
     "iopub.status.busy": "2023-02-28T11:04:57.665726Z",
     "iopub.status.idle": "2023-02-28T11:04:58.213790Z",
     "shell.execute_reply": "2023-02-28T11:04:58.212790Z"
    }
   },
   "outputs": [],
   "source": [
    "coarse_to_fine_array = np.zeros(shape=[number_of_classes_c,number_of_classes_f], dtype=np.int32)\n",
    "\n",
    "c_id = np.argmax(dataset['y_train_coarse'],1)\n",
    "\n",
    "f_id = np.argmax(dataset['y_train_fine'],1)\n",
    "\n",
    "for x in range(len(dataset['y_train_fine'])):\n",
    "    coarse_to_fine_array[c_id[x]][f_id[x]] = 1\n",
    "\n",
    "Matrix_coarse_to_fine_OneHot = tf.constant(coarse_to_fine_array, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796bab2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:04:58.229723Z",
     "iopub.status.busy": "2023-02-28T11:04:58.229723Z",
     "iopub.status.idle": "2023-02-28T11:04:58.555789Z",
     "shell.execute_reply": "2023-02-28T11:04:58.554729Z"
    }
   },
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_proba):\n",
    "    \n",
    "    present_error_raw = tf.square(tf.maximum(0., 0.9 - y_proba), name=\"present_error_raw\")\n",
    "    absent_error_raw = tf.square(tf.maximum(0., y_proba - 0.1), name=\"absent_error_raw\")\n",
    "    L = tf.add(y_true * present_error_raw, 0.5 * (1.0 - y_true) * absent_error_raw,name=\"L\")\n",
    "    total_marginloss = tf.reduce_sum(L, axis=1, name=\"margin_loss\")\n",
    "\n",
    "    return total_marginloss\n",
    "\n",
    "\n",
    "def consistency_check(y_pred_ancestor,y_pred_current,lookup_matrix,num_class_current):\n",
    "    pred_max_ancestor = tf.argmax(y_pred_ancestor,axis=1)\n",
    "    pred_max_current = tf.argmax(y_pred_current,axis=1)\n",
    "    \n",
    "    consistant_check = tf.gather(lookup_matrix, indices=pred_max_ancestor)*tf.one_hot(pred_max_current,num_class_current)\n",
    "    \n",
    "    return tf.reduce_sum(consistant_check,1)\n",
    "\n",
    "def get_consistency(y_true_ancestor, y_pred, lookup_matrix):\n",
    "    '''\n",
    "    Get consistency based on 2 levels\n",
    "    Provide ture levels for the level above, predictions for the current level and a look up metrix\n",
    "    '''\n",
    "    y_prob = tf.math.divide(y_pred,tf.reshape(tf.reduce_sum(y_pred,-1),(-1,1),name='reshape'),name='Probability')\n",
    "    \n",
    "    index_for_predictions = tf.cast(tf.math.argmax(y_true_ancestor,axis=1),dtype= 'int32')\n",
    "    consistent_fine = tf.gather(lookup_matrix, indices=index_for_predictions) * y_prob\n",
    "    Consistency_sum_array = tf.reduce_sum(consistent_fine, axis =1)\n",
    "    \n",
    "    return tf.abs(1-Consistency_sum_array)\n",
    "\n",
    "def CustomLoss(y_true_c, y_true_f, y_pred_c, y_pred_f, LW_C, LW_F,\n",
    "               number_of_classes_f, C_Weight=0.2):\n",
    "    \n",
    "    con_f = consistency_check(y_pred_c,y_pred_f,Matrix_coarse_to_fine_OneHot,num_class_current=number_of_classes_f)\n",
    "    con_f_not = tf.abs(con_f-1)\n",
    "    \n",
    "    con_sum_f = get_consistency(y_true_c,y_pred_f,Matrix_coarse_to_fine_OneHot)\n",
    "    \n",
    "    fine_lvl_cosistency = con_sum_f * con_f_not    \n",
    "   \n",
    "    ML_c = LW_C*(margin_loss(y_true_c, y_pred_c))\n",
    "    ML_f = LW_F*(margin_loss(y_true_f, y_pred_f))\n",
    "    consistency_loss = C_Weight*(fine_lvl_cosistency)\n",
    "    \n",
    "    batch_loss = ML_c + ML_f + consistency_loss\n",
    "\n",
    "    return tf.reduce_mean(batch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a497572",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935c049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:04:58.570725Z",
     "iopub.status.busy": "2023-02-28T11:04:58.569721Z",
     "iopub.status.idle": "2023-02-28T11:04:58.970983Z",
     "shell.execute_reply": "2023-02-28T11:04:58.972983Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_compiled_model():\n",
    "    \n",
    "    ## Calling the HD-CapsNet Model\n",
    "    model = models.HD_CapsNet_Mod_3_2(input_shape     = dataset['x_train'].shape[1:], \n",
    "                                      input_shape_yc  = dataset['y_train_coarse'].shape[1:],\n",
    "                                      input_shape_yf  = dataset['y_train_fine'].shape[1:], \n",
    "                                      no_coarse_class = number_of_classes_c,\n",
    "                                      no_fine_class   = number_of_classes_f,\n",
    "                                      PCap_n_dims     = 8, \n",
    "                                      SCap_f_dims     = 16,\n",
    "                                      SCap_c_dims     = 32\n",
    "                                     )\n",
    "    \n",
    "    ## Saving Model Architecture\n",
    "    keras.utils.plot_model(model, to_file = directory+\"/Architecture.png\", show_shapes=True)\n",
    "    \n",
    "    ## Add Loss for Model\n",
    "    model.add_loss(CustomLoss(y_true_c            = model.inputs[1],\n",
    "                              y_true_f            = model.inputs[2], \n",
    "                              y_pred_c            = model.output[0],\n",
    "                              y_pred_f            = model.output[1], \n",
    "                              LW_C                = lossweight['coarse_lw'],\n",
    "                              LW_F                = lossweight['fine_lw'],\n",
    "                              number_of_classes_f = number_of_classes_f, \n",
    "                              C_Weight            = lossweight['consistency_lw']\n",
    "                             )\n",
    "                  )\n",
    "    \n",
    "    ## Compile Model\n",
    "    model.compile(optimizer='adam',                  \n",
    "                  metrics={'prediction_fine': 'accuracy',\n",
    "                           'prediction_coarse': 'accuracy'}\n",
    "                 )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86393b7d",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19234f8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:04:58.979916Z",
     "iopub.status.busy": "2023-02-28T11:04:58.978985Z",
     "iopub.status.idle": "2023-02-28T11:05:02.511938Z",
     "shell.execute_reply": "2023-02-28T11:05:02.513942Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fafa5af",
   "metadata": {},
   "source": [
    "strategy = models.multi_gpu_select('windows')\n",
    "\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = get_compiled_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4b982",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:05:02.519943Z",
     "iopub.status.busy": "2023-02-28T11:05:02.519943Z",
     "iopub.status.idle": "2023-02-28T11:05:03.123037Z",
     "shell.execute_reply": "2023-02-28T11:05:03.114038Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f72ece",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f28f4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:05:03.128035Z",
     "iopub.status.busy": "2023-02-28T11:05:03.128035Z",
     "iopub.status.idle": "2023-02-28T11:05:03.580595Z",
     "shell.execute_reply": "2023-02-28T11:05:03.581591Z"
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                            height_shift_range=0.1)\n",
    "\n",
    "training_generator = MixUp_add_loss.MixupGenerator_2level(dataset['x_train'],\n",
    "                                                 dataset['y_train_coarse'],\n",
    "                                                 dataset['y_train_fine'],\n",
    "                                                 batch_size=train_params[\"batch_size\"],\n",
    "                                                 alpha=0.2, \n",
    "                                                 datagen=datagen\n",
    "                                                )()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67d719",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a123c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:05:03.593600Z",
     "iopub.status.busy": "2023-02-28T11:05:03.591595Z",
     "iopub.status.idle": "2023-02-28T11:05:04.044589Z",
     "shell.execute_reply": "2023-02-28T11:05:04.045589Z"
    }
   },
   "outputs": [],
   "source": [
    "tb = keras.callbacks.TensorBoard(directory+'./tb_logs'+ datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "log = keras.callbacks.CSVLogger(directory+'/log.csv', append=True)\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(directory+'/epoch-best.h5',\n",
    "                                             monitor='val_prediction_fine_accuracy',\n",
    "                                             save_best_only=True, \n",
    "                                             save_weights_only=True, \n",
    "                                             verbose=1)\n",
    "\n",
    "change_lw = models.LossWeightsModifier(lossweight = lossweight,\n",
    "                                       initial_lw = initial_lw,\n",
    "                                       directory = directory)\n",
    "\n",
    "lr_decay = keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc176a2",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4a0be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:05:04.056591Z",
     "iopub.status.busy": "2023-02-28T11:05:04.054588Z",
     "iopub.status.idle": "2023-02-28T11:05:04.711864Z",
     "shell.execute_reply": "2023-02-28T11:05:04.710863Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_save_dir = str(directory+'/trained_model.h5')\n",
    "try:\n",
    "    model.load_weights(model_save_dir)\n",
    "except:\n",
    "    history = model.fit(training_generator,\n",
    "                        steps_per_epoch = int(dataset['x_train'].shape[0] / train_params[\"batch_size\"]),\n",
    "                        epochs = train_params[\"n_epochs\"],\n",
    "                        validation_data = ([dataset['x_test'], dataset['y_test_coarse'], dataset['y_test_fine']],\n",
    "                                           [dataset['y_test_coarse'],dataset['y_test_fine']]\n",
    "                                          ),\n",
    "                        callbacks = [tb,log,change_lw,lr_decay,checkpoint],\n",
    "                        verbose=1\n",
    "                       )\n",
    "    \n",
    "    model.save_weights(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d7b95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:05:04.721864Z",
     "iopub.status.busy": "2023-02-28T11:05:04.720866Z",
     "iopub.status.idle": "2023-02-28T11:05:05.186147Z",
     "shell.execute_reply": "2023-02-28T11:05:05.184152Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    history_dict = history.history\n",
    "\n",
    "    plotter = tfdocs.plots.HistoryPlotter()\n",
    "    plotter.plot({\"Coarse\": history}, metric = \"prediction_coarse_accuracy\")\n",
    "    plotter.plot({\"Fine\": history}, metric = \"prediction_fine_accuracy\")\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.ylim([0,1])\n",
    "except:\n",
    "    print('Trained model weights loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f47f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:05:05.194146Z",
     "iopub.status.busy": "2023-02-28T11:05:05.192149Z",
     "iopub.status.idle": "2023-02-28T11:05:05.582696Z",
     "shell.execute_reply": "2023-02-28T11:05:05.583696Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    plotter = tfdocs.plots.HistoryPlotter()\n",
    "    plotter.plot({\"loss\": history}, metric = \"loss\")\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.ylim([0,1])\n",
    "except:\n",
    "    print('Trained model weights loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08f53b",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6652e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:05:05.592700Z",
     "iopub.status.busy": "2023-02-28T11:05:05.591699Z",
     "iopub.status.idle": "2023-02-28T11:05:05.968866Z",
     "shell.execute_reply": "2023-02-28T11:05:05.969867Z"
    }
   },
   "outputs": [],
   "source": [
    "final_model = keras.Model(model.inputs[:1], model.output)\n",
    "\n",
    "lossfn = models.MarginLoss()\n",
    "final_model.compile(optimizer='adam', \n",
    "                    loss={'prediction_fine' : lossfn,\n",
    "                          'prediction_coarse' : lossfn},\n",
    "\n",
    "                    loss_weights={'prediction_fine' : lossweight['fine_lw'],\n",
    "                                  'prediction_coarse' : lossweight['coarse_lw']\n",
    "                                 },\n",
    "\n",
    "                    metrics={'prediction_fine': 'accuracy',\n",
    "                             'prediction_coarse': 'accuracy'\n",
    "                            }\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed4e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:05:05.977866Z",
     "iopub.status.busy": "2023-02-28T11:05:05.976868Z",
     "iopub.status.idle": "2023-02-28T11:05:27.668682Z",
     "shell.execute_reply": "2023-02-28T11:05:27.667682Z"
    }
   },
   "outputs": [],
   "source": [
    "model_analysis = models.model_analysis(final_model, dataset)\n",
    "results = model_analysis.evaluate()\n",
    "predictions = model_analysis.prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67911a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:05:27.676681Z",
     "iopub.status.busy": "2023-02-28T11:05:27.675682Z",
     "iopub.status.idle": "2023-02-28T11:05:35.279671Z",
     "shell.execute_reply": "2023-02-28T11:05:35.279671Z"
    }
   },
   "outputs": [],
   "source": [
    "true_label = [dataset['y_test_coarse'],dataset['y_test_fine']]\n",
    "pred_label = [predictions[0],predictions[1]]\n",
    "metrics.lvl_wise_metric(true_label,pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb4db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T11:05:35.285671Z",
     "iopub.status.busy": "2023-02-28T11:05:35.285671Z",
     "iopub.status.idle": "2023-02-28T11:05:36.408705Z",
     "shell.execute_reply": "2023-02-28T11:05:36.409704Z"
    }
   },
   "outputs": [],
   "source": [
    "h_measurements,consistency,exact_match = metrics.hmeasurements(true_label,\n",
    "                                                               pred_label,\n",
    "                                                               dataset['tree']\n",
    "                                                              )\n",
    "print('\\nHierarchical Precision =',h_measurements[0],\n",
    "      '\\nHierarchical Recall =', h_measurements[1],\n",
    "      '\\nHierarchical F1-Score =',h_measurements[2],\n",
    "      '\\nConsistency = ', consistency,\n",
    "      '\\nExact Match = ', exact_match,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b98d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py38tf] *",
   "language": "python",
   "name": "conda-env-.conda-py38tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
