{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b24df6d",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"6\"><center><b> HD-CapsNet: A Hierarchical Deep Capsule Network for Image Classification </b></center></font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691c83f",
   "metadata": {},
   "source": [
    "**Changing Model Architecture**\n",
    "- **(Mod-3.3)** 64-D>32-D>16-D (Coarse>Medium>FINE) use skip connections between Secondary Capsules $Concatenate([P_{caps}, S_{coarse}])$ > input for $S_{medium}$ and $Concatenate([P_{caps}, S_{medium}])$ > input for $S_{fine}$\n",
    "- With $L_{Cons}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9d6d5",
   "metadata": {},
   "source": [
    "# Files and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4aa737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "# Supporting Libraries:\n",
    "    #Mathplot lib for ploting graphs\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "    # numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "    #system\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "    #import other libraries\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "from treelib import Tree\n",
    "    # ML model, Dataset and evalution metrics\n",
    "sys.path.append('../../') ### adding system parth for src folder\n",
    "from src import datasets # load datasets\n",
    "from src import MixUp # load datasets\n",
    "from src import MixUp_add_loss # load datasets\n",
    "from src import metrics # load hierarchcial metrics\n",
    "from src import sysenv # load hierarchcial metrics\n",
    "from src import models # load machine learning models\n",
    "\n",
    "    ## Tensorflow_docs\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "    # Auto reload local libraries if updated\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c610b",
   "metadata": {},
   "source": [
    "# System information & GPU growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "systeminfo = sysenv.systeminfo()\n",
    "print(systeminfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = \"0,1,2,3,4,5,6,7\" ## Selecting Available gpus (Multi-GPUS)\n",
    "gpus = \"5,6\" ## Selecting Available gpus (Single GPU)\n",
    "gpugrowth = sysenv.gpugrowth(gpus = gpus) ## Limiting GPUS from OS environment\n",
    "gpugrowth.memory_growth() #GPU memory growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd551837",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445ee1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\"n_epochs\" : 100,\n",
    "                \"batch_size\": 16,\n",
    "                \"lr\": 0.001, # Initial learning rate\n",
    "                \"lr_decay\": 0.95, # Learning rate decay\n",
    "                \"decay_exe\": 9, #learning rate decay execution epoch after\n",
    "               }\n",
    "model_params = {\"optimizer\": tf.keras.optimizers.Adam(train_params['lr']),\n",
    "                \"loss_function\": models.MarginLoss(),\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98815c",
   "metadata": {},
   "source": [
    "# log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ace371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory = sysenv.log_dir('5_CU_Bird/HD_CapsNet/Mod_3_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfde972",
   "metadata": {},
   "source": [
    "# Import Dataset : CU BIRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset,  test_dataset, val_dataset, tree, info = datasets.CU_Birds_200_2011(image_size=(64, 64), \n",
    "                                                                                  batch_size=train_params['batch_size'],\n",
    "                                                                                  data_aug = 'mixup', # 'mixup'\n",
    "                                                                                  data_normalizing = None) #'normalize'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in training_dataset.take(1):\n",
    "    image_shape = x.shape[1:]\n",
    "    coarse_label_shape = y[0].shape[1:]\n",
    "    medium_label_shape = y[1].shape[1:]\n",
    "    fine_label_shape = y[2].shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38ee07",
   "metadata": {},
   "source": [
    "## Learning Rate Decay Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed77d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    learning_rate_init = train_params[\"lr\"]\n",
    "    \n",
    "    if epoch > train_params[\"decay_exe\"]:\n",
    "        learning_rate_init = train_params[\"lr\"] * (train_params[\"lr_decay\"] ** (epoch-9))\n",
    "        \n",
    "    tf.summary.scalar('learning rate', data=learning_rate_init, step=epoch)\n",
    "        \n",
    "    return learning_rate_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5ea00",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee6349a",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48854261",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Dynamic LossWeights\n",
    "initial_lw = models.initial_lw({\"coarse\": info['Num_Coarse'],\n",
    "                                \"medium\": info['Num_Medium'],\n",
    "                                \"fine\": info['Num_Fine']})\n",
    "\n",
    "lossweight = {'coarse_lw' : K.variable(value = initial_lw['coarse'], dtype=\"float32\", name=\"coarse_lw\"),\n",
    "             'medium_lw' : K.variable(value = initial_lw['medium'], dtype=\"float32\", name=\"medium_lw\"),\n",
    "             'fine_lw' : K.variable(value = initial_lw['fine'], dtype=\"float32\", name=\"fine_lw\"),\n",
    "              'decoder_lw' : 0.0\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a92ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix_coarse_to_medium_OneHot = tf.constant(info['Matrix_coarse_to_medium'], dtype=tf.float32)\n",
    "Matrix_medium_to_fine_OneHot = tf.constant(info['Matrix_medium_to_fine'], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796bab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_proba):\n",
    "    \n",
    "    present_error_raw = tf.square(tf.maximum(0., 0.9 - y_proba), name=\"present_error_raw\")\n",
    "    absent_error_raw = tf.square(tf.maximum(0., y_proba - 0.1), name=\"absent_error_raw\")\n",
    "    L = tf.add(y_true * present_error_raw, 0.5 * (1.0 - y_true) * absent_error_raw,name=\"L\")\n",
    "    total_marginloss = tf.reduce_sum(L, axis=1, name=\"margin_loss\")\n",
    "\n",
    "    return total_marginloss\n",
    "\n",
    "\n",
    "def consistency_check(y_pred_ancestor,y_pred_current,lookup_matrix,num_class_current):\n",
    "    pred_max_ancestor = tf.argmax(y_pred_ancestor,axis=1)\n",
    "    pred_max_current = tf.argmax(y_pred_current,axis=1)\n",
    "    \n",
    "    consistant_check = tf.gather(lookup_matrix, indices=pred_max_ancestor)*tf.one_hot(pred_max_current,num_class_current)\n",
    "    \n",
    "    return tf.reduce_sum(consistant_check,1)\n",
    "\n",
    "def get_consistency(y_true_ancestor, y_pred, lookup_matrix):\n",
    "    '''\n",
    "    Get consistency based on 2 levels\n",
    "    Provide ture levels for the level above, predictions for the current level and a look up metrix\n",
    "    '''\n",
    "    y_prob = tf.math.divide(y_pred,tf.reshape(tf.reduce_sum(y_pred,-1),(-1,1),name='reshape'),name='Probability')\n",
    "    \n",
    "    index_for_predictions = tf.cast(tf.math.argmax(y_true_ancestor,axis=1),dtype= 'int32')\n",
    "    consistent_fine = tf.gather(lookup_matrix, indices=index_for_predictions) * y_prob\n",
    "    Consistency_sum_array = tf.reduce_sum(consistent_fine, axis =1)\n",
    "    \n",
    "    return tf.abs(1-Consistency_sum_array)\n",
    "\n",
    "def CustomLoss(y_true_c, y_true_m, y_true_f, y_pred_c, y_pred_m, y_pred_f, LW_C, LW_M, LW_F,\n",
    "               number_of_classes_m, number_of_classes_f, C_Weight=0.2):\n",
    "    \n",
    "    con_m = consistency_check(y_pred_c,y_pred_m,Matrix_coarse_to_medium_OneHot,num_class_current=number_of_classes_m)\n",
    "    con_m_not = tf.abs(con_m-1)\n",
    "    \n",
    "    con_f = consistency_check(y_pred_m,y_pred_f,Matrix_medium_to_fine_OneHot,num_class_current=number_of_classes_f)\n",
    "    con_f_not = tf.abs(con_f-1)\n",
    "    \n",
    "    con_sum_m = get_consistency(y_true_c,y_pred_m,Matrix_coarse_to_medium_OneHot)\n",
    "    con_sum_f = get_consistency(y_true_m,y_pred_f,Matrix_medium_to_fine_OneHot)\n",
    "    \n",
    "    medium_lvl_cosistency = con_sum_m * con_m_not\n",
    "    fine_lvl_cosistency = con_sum_f * con_f_not    \n",
    "   \n",
    "    ML_c = margin_loss(y_true_c, y_pred_c)*LW_C\n",
    "    ML_m = LW_M*((1-C_Weight)*(margin_loss(y_true_m, y_pred_m))+C_Weight*(medium_lvl_cosistency))\n",
    "    ML_f = LW_F*((1-C_Weight)*(margin_loss(y_true_f, y_pred_f))+C_Weight*(fine_lvl_cosistency))\n",
    "    \n",
    "    batch_loss = ML_c + ML_m+ ML_f\n",
    "\n",
    "    return tf.reduce_mean(batch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a497572",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model():\n",
    "    \n",
    "    ## Calling the HD-CapsNet Model\n",
    "    model = models.HD_CapsNet_Mod_3_3(input_shape     = image_shape, \n",
    "                                      input_shape_yc  = coarse_label_shape,\n",
    "                                      input_shape_ym  = medium_label_shape,\n",
    "                                      input_shape_yf  = fine_label_shape, \n",
    "                                      no_coarse_class = info['Num_Coarse'], \n",
    "                                      no_medium_class = info['Num_Medium'], \n",
    "                                      no_fine_class   = info['Num_Fine'],\n",
    "                                      PCap_n_dims     = 8, \n",
    "                                      SCap_f_dims     = 16, \n",
    "                                      SCap_m_dims     = 32, \n",
    "                                      SCap_c_dims     = 64)\n",
    "    \n",
    "    ## Saving Model Architecture\n",
    "    keras.utils.plot_model(model, to_file = directory+\"/Architecture.png\", show_shapes=True)\n",
    "    \n",
    "    ## Add Loss for Model\n",
    "    model.add_loss(CustomLoss(y_true_c            = model.inputs[1], \n",
    "                              y_true_m            = model.inputs[2], \n",
    "                              y_true_f            = model.inputs[3], \n",
    "                              y_pred_c            = model.output[0], \n",
    "                              y_pred_m            = model.output[1], \n",
    "                              y_pred_f            = model.output[2], \n",
    "                              LW_C                = lossweight['coarse_lw'], \n",
    "                              LW_M                = lossweight['medium_lw'], \n",
    "                              LW_F                = lossweight['fine_lw'],\n",
    "                              number_of_classes_m = info['Num_Medium'], \n",
    "                              number_of_classes_f = info['Num_Fine'], \n",
    "                              C_Weight            =0.2)\n",
    "                  )\n",
    "    \n",
    "    ## Compile Model\n",
    "    model.compile(optimizer='adam',                  \n",
    "                  metrics={'prediction_fine': 'accuracy',\n",
    "                           'prediction_medium': 'accuracy',\n",
    "                           'prediction_coarse': 'accuracy'}\n",
    "                 )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86393b7d",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2d184b9",
   "metadata": {},
   "source": [
    "model = get_compiled_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = models.multi_gpu_select('windows')\n",
    "\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = get_compiled_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4b982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f72ece",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e1851d4",
   "metadata": {},
   "source": [
    "### Adding Mixup Data Augmentation\n",
    "training_dataset_MU = MixUp.data_mixup_3_labels(training_dataset,\n",
    "                                                alpha=0.2, batch_size = train_params['batch_size'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8078d887",
   "metadata": {},
   "source": [
    "## Training Pipeline Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d82a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Matching the pipeling with model inputs\n",
    "def pipeline_multi_input_output(image, label):\n",
    "    label_0 = label[0]\n",
    "    label_1 = label[1]\n",
    "    label_2 = label[2]\n",
    "    return (image, label_0, label_1, label_2), (label_0, label_1, label_2)\n",
    "\n",
    "training_dataset_match = training_dataset.map(pipeline_multi_input_output) ## Mixup dataset\n",
    "val_dataset_match = val_dataset.map(pipeline_multi_input_output) ## Val Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in training_dataset_match.take(5):\n",
    "    plt.imshow(x[0][0])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67d719",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = keras.callbacks.TensorBoard(directory+'./tb_logs'+ datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "log = keras.callbacks.CSVLogger(directory+'/log.csv', append=True)\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(directory+'/epoch-best.h5',\n",
    "                                             monitor='val_prediction_fine_accuracy',\n",
    "                                             save_best_only=True, \n",
    "                                             save_weights_only=True, \n",
    "                                             verbose=1)\n",
    "\n",
    "change_lw = models.LossWeightsModifier(lossweight = lossweight,\n",
    "                                       initial_lw = initial_lw,\n",
    "                                       directory = directory)\n",
    "\n",
    "lr_decay = keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc176a2",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4a0be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_save_dir = str(directory+'/trained_model.h5')\n",
    "try:\n",
    "    model.load_weights(model_save_dir)\n",
    "except:\n",
    "    history = model.fit(training_dataset_match,\n",
    "                        epochs = train_params[\"n_epochs\"],\n",
    "                        validation_data = val_dataset_match,\n",
    "                        callbacks = [tb,log,change_lw,lr_decay,checkpoint],\n",
    "                        verbose=1)\n",
    "    \n",
    "    model.save_weights(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    history_dict = history.history\n",
    "\n",
    "    plotter = tfdocs.plots.HistoryPlotter()\n",
    "    plotter.plot({\"Coarse\": history}, metric = \"prediction_coarse_accuracy\")\n",
    "    plotter.plot({\"Medium\": history}, metric = \"prediction_medium_accuracy\")\n",
    "    plotter.plot({\"Fine\": history}, metric = \"prediction_fine_accuracy\")\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.ylim([0,1])\n",
    "except:\n",
    "    print('Trained model weights loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f47f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plotter = tfdocs.plots.HistoryPlotter()\n",
    "    plotter.plot({\"loss\": history}, metric = \"loss\")\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.ylim([0,1])\n",
    "except:\n",
    "    print('Trained model weights loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08f53b",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6652e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = keras.Model(model.inputs[:1], model.output)\n",
    "\n",
    "lossfn = models.MarginLoss()\n",
    "final_model.compile(optimizer='adam', \n",
    "                    loss={'prediction_fine' : lossfn,\n",
    "                          'prediction_medium' : lossfn,\n",
    "                          'prediction_coarse' : lossfn},\n",
    "\n",
    "                    loss_weights={'prediction_fine' : lossweight['fine_lw'],\n",
    "                                  'prediction_medium' : lossweight['medium_lw'],\n",
    "                                  'prediction_coarse' : lossweight['coarse_lw']},\n",
    "\n",
    "                    metrics={'prediction_fine': 'accuracy',\n",
    "                             'prediction_medium': 'accuracy',\n",
    "                             'prediction_coarse': 'accuracy'\n",
    "                            }\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = final_model.evaluate(test_dataset)\n",
    "for n in range(len(results)):\n",
    "    print(str(n+1)+'.',final_model.metrics_names[n], '==>', results[n])\n",
    "model_predictions = final_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67911a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_pipeline(model, dataset):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for x, y in dataset:\n",
    "        batch_pred = model.predict(x)\n",
    "        for i in range(len(batch_pred)):\n",
    "            if i >= len(y_pred):\n",
    "                y_pred.append(None)\n",
    "                y_true.append(None)\n",
    "            if y_pred[i] is None:\n",
    "                y_pred[i] = batch_pred[i]\n",
    "                y_true[i] = list(y[i].numpy())\n",
    "            else:\n",
    "                y_pred[i] = np.concatenate([y_pred[i], batch_pred[i]])\n",
    "                y_true[i] = y_true[i] + list(y[i].numpy())\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb4db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "predictions.append(predict_from_pipeline(final_model, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74433ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= {'y_test_coarse':predictions[0][0][0], \n",
    "          'y_test_medium':predictions[0][0][1], \n",
    "          'y_test_fine':predictions[0][0][2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5562d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = [dataset['y_test_coarse'],dataset['y_test_medium'],dataset['y_test_fine']]\n",
    "pred_label = [predictions[0][1][0],predictions[0][1][1],predictions[0][1][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8e211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.lvl_wise_metric(true_label,pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_measurements,consistency,exact_match = metrics.hmeasurements(true_label,pred_label,tree)\n",
    "print('\\nHierarchical Precision =',h_measurements[0],\n",
    "      '\\nHierarchical Recall =', h_measurements[1],\n",
    "      '\\nHierarchical F1-Score =',h_measurements[2],\n",
    "      '\\nConsistency = ', consistency,\n",
    "      '\\nExact Match = ', exact_match,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b98d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
