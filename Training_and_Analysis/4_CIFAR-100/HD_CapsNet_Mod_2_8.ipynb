{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b24df6d",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"6\"><center><b> HD-CapsNet: A Hierarchical Deep Capsule Network for Image Classification </b></center></font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec310841",
   "metadata": {},
   "source": [
    "**Changing Model Architecture**\n",
    "- **(Mod-2.8)** From coarse-to-Fine slowly decrease the dimension. i.e. 32D>24D>16D (Coarse>Medium>FINE), While keeping the primary capsule dimension same as original ($P_{capsule} = 8D$). Training without $L_{cons}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9d6d5",
   "metadata": {},
   "source": [
    "# Files and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4aa737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "# Supporting Libraries:\n",
    "    #Mathplot lib for ploting graphs\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "    # numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "    #system\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "    #import other libraries\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "from treelib import Tree\n",
    "    # ML model, Dataset and evalution metrics\n",
    "sys.path.append('../../') ### adding system parth for src folder\n",
    "from src import datasets # load datasets\n",
    "from src import MixUp_add_loss # load datasets\n",
    "from src import metrics # load hierarchcial metrics\n",
    "from src import sysenv # load hierarchcial metrics\n",
    "from src import models # load machine learning models\n",
    "\n",
    "    ## Tensorflow_docs\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "    # Auto reload local libraries if updated\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c610b",
   "metadata": {},
   "source": [
    "# System information & GPU growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "systeminfo = sysenv.systeminfo()\n",
    "print(systeminfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = \"0,1,2,3,4,5,6,7\" ## Selecting Available gpus (Multi-GPUS)\n",
    "gpus = \"3\" ## Selecting Available gpus (Single GPU)\n",
    "gpugrowth = sysenv.gpugrowth(gpus = gpus) ## Limiting GPUS from OS environment\n",
    "gpugrowth.memory_growth() #GPU memory growth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3136b6e",
   "metadata": {},
   "source": [
    "## log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = sysenv.log_dir('4_CIFAR_100/HD_CapsNet/Mod_2_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfde972",
   "metadata": {},
   "source": [
    "# Import Dataset : CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.CIFAR100(version = 'ALL') # importing CIFAR10 Dataset\n",
    "# dataset = datasets.CIFAR100(version = 'reduce') # importing CIFAR10 Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd551837",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\"n_epochs\" : 100,\n",
    "                \"batch_size\": 64,\n",
    "                \"lr\": 0.001, # Initial learning rate\n",
    "                \"lr_decay\": 0.95, # Learning rate decay\n",
    "                \"decay_exe\": 9, #learning rate decay execution epoch after\n",
    "               }\n",
    "model_params = {\"optimizer\": tf.keras.optimizers.Adam(train_params['lr']),\n",
    "                \"loss_function\": models.MarginLoss(),\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38ee07",
   "metadata": {},
   "source": [
    "## Learning Rate Decay Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed77d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    learning_rate_init = train_params[\"lr\"]\n",
    "    \n",
    "    if epoch > train_params[\"decay_exe\"]:\n",
    "        learning_rate_init = train_params[\"lr\"] * (train_params[\"lr_decay\"] ** (epoch-9))\n",
    "        \n",
    "    tf.summary.scalar('learning rate', data=learning_rate_init, step=epoch)\n",
    "        \n",
    "    return learning_rate_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9272c71",
   "metadata": {},
   "source": [
    "# Bottom up Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d5e21",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48854261",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes_c = len(np.unique(np.argmax(dataset['y_train_coarse'], axis=1)))\n",
    "number_of_classes_m = len(np.unique(np.argmax(dataset['y_train_medium'], axis=1)))\n",
    "number_of_classes_f = len(np.unique(np.argmax(dataset['y_train_fine'], axis=1)))\n",
    "\n",
    "## For Dynamic LossWeights\n",
    "initial_lw = models.initial_lw({\"coarse\": number_of_classes_c,\n",
    "                                \"medium\": number_of_classes_m,\n",
    "                                \"fine\": number_of_classes_f})\n",
    "\n",
    "lossweight = {'coarse_lw' : K.variable(value = initial_lw['coarse'], dtype=\"float32\", name=\"coarse_lw\"),\n",
    "             'medium_lw' : K.variable(value = initial_lw['medium'], dtype=\"float32\", name=\"medium_lw\"),\n",
    "             'fine_lw' : K.variable(value = initial_lw['fine'], dtype=\"float32\", name=\"fine_lw\"),\n",
    "              'decoder_lw' : 0.0\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256fd873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_proba):\n",
    "    \n",
    "    present_error_raw = tf.square(tf.maximum(0., 0.9 - y_proba), name=\"present_error_raw\")\n",
    "    absent_error_raw = tf.square(tf.maximum(0., y_proba - 0.1), name=\"absent_error_raw\")\n",
    "    L = tf.add(y_true * present_error_raw, 0.5 * (1.0 - y_true) * absent_error_raw,name=\"L\")\n",
    "    total_marginloss = tf.reduce_sum(L, axis=1, name=\"margin_loss\")\n",
    "    return total_marginloss\n",
    "\n",
    "def CustomLoss(y_true_c, y_true_m, y_true_f, y_pred_c, y_pred_m, y_pred_f, LW_C, LW_M, LW_F):\n",
    "    ML_c = margin_loss(y_true_c, y_pred_c)*LW_C\n",
    "    ML_m = LW_M*margin_loss(y_true_m, y_pred_m)\n",
    "    ML_f = LW_F*margin_loss(y_true_f, y_pred_f)\n",
    "    batch_loss = ML_c + ML_m+ ML_f\n",
    "    return tf.reduce_mean(batch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5ea00",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91672ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model():\n",
    "    input_shape = dataset['x_train'].shape[1:]\n",
    "\n",
    "    input_shape_yc = dataset['y_train_coarse'].shape[1:]\n",
    "    input_shape_ym = dataset['y_train_medium'].shape[1:]\n",
    "    input_shape_yf = dataset['y_train_fine'].shape[1:]\n",
    "\n",
    "    no_coarse_class = number_of_classes_c\n",
    "    no_medium_class = number_of_classes_m\n",
    "    no_fine_class = number_of_classes_f\n",
    "\n",
    "    PCap_n_dims = 8\n",
    "\n",
    "    SCap_f_dims = 16\n",
    "    SCap_m_dims = 24\n",
    "    SCap_c_dims = 32\n",
    "\n",
    "\n",
    "    # Input image\n",
    "    x_input = keras.layers.Input(shape=input_shape, name=\"Input_Image\")\n",
    "\n",
    "    # Input True Labels\n",
    "    y_c = keras.layers.Input(shape=input_shape_yc, name='input_yc')\n",
    "    y_m = keras.layers.Input(shape=input_shape_ym, name='input_ym')\n",
    "    y_f = keras.layers.Input(shape=input_shape_yf, name='input_yf')\n",
    "\n",
    "    #--- block 1 ---\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(x_input)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    #--- block 2 ---\n",
    "    x = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    #--- block 3 ---\n",
    "    x = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    #--- block 4 ---\n",
    "    x = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "\n",
    "    # Layer 3: Reshape to 8D primary capsules \n",
    "    reshapec = keras.layers.Reshape((int((tf.reduce_prod(x.shape[1:]).numpy())/PCap_n_dims),\n",
    "                                     PCap_n_dims), name=\"reshape_layer\")(x)\n",
    "    p_caps = keras.layers.Lambda(models.squash, name='p_caps')(reshapec)\n",
    "\n",
    "    ## Layer Secondary Capsule: For coarse level\n",
    "    s_caps_c = models.SecondaryCapsule(n_caps=no_coarse_class, n_dims=SCap_c_dims, \n",
    "                        name=\"s_caps_coarse\")(p_caps)\n",
    "    \n",
    "    ## Layer Secondary Capsule: For medium level\n",
    "    s_caps_m = models.SecondaryCapsule(n_caps=no_medium_class, n_dims=SCap_m_dims, \n",
    "                        name=\"s_caps_medium\")(s_caps_c)\n",
    "\n",
    "    ## Layer Secondary Capsule: For fine level\n",
    "    s_caps_f = models.SecondaryCapsule(n_caps=no_fine_class, n_dims=SCap_f_dims, \n",
    "                        name=\"s_caps_fine\")(s_caps_m)\n",
    "\n",
    "    pred_c = models.LengthLayer(name='prediction_coarse')(s_caps_c)\n",
    "\n",
    "    pred_m = models.LengthLayer(name='prediction_medium')(s_caps_m)\n",
    "\n",
    "    pred_f = models.LengthLayer(name='prediction_fine')(s_caps_f)\n",
    "\n",
    "    model = keras.Model(inputs= [x_input, y_c, y_m, y_f],\n",
    "                        outputs= [pred_c, pred_m, pred_f],\n",
    "                        name='HD-CapsNet')\n",
    "\n",
    "    model.add_loss(CustomLoss(y_c, y_m, y_f, pred_c, pred_m, pred_f, \n",
    "                              lossweight['coarse_lw'], lossweight['medium_lw'], lossweight['fine_lw']))\n",
    "\n",
    "    model.compile(optimizer='adam',                  \n",
    "                  metrics={'prediction_fine': 'accuracy',\n",
    "                           'prediction_medium': 'accuracy',\n",
    "                           'prediction_coarse': 'accuracy'\n",
    "                          }\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1087df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_compiled_model()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17aadb80",
   "metadata": {},
   "source": [
    "strategy = multi_gpu_select('windows')\n",
    "\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = get_compiled_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4b982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "# keras.utils.plot_model(model, to_file = directory+\"/Architecture.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f72ece",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857527ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                            height_shift_range=0.1)\n",
    "\n",
    "training_generator = MixUp_add_loss.MixupGenerator_3level(dataset['x_train'],\n",
    "                                                 dataset['y_train_coarse'], \n",
    "                                                 dataset['y_train_medium'],\n",
    "                                                 dataset['y_train_fine'],\n",
    "                                                 batch_size=train_params[\"batch_size\"],\n",
    "                                                 alpha=0.2, \n",
    "                                                 datagen=datagen\n",
    "                                                )()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67d719",
   "metadata": {},
   "source": [
    "## Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = keras.callbacks.TensorBoard(directory+'./tb_logs'+ datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "log = keras.callbacks.CSVLogger(directory+'/log.csv', append=True)\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    directory+'/epoch-best.h5', \n",
    "    monitor='val_prediction_fine_accuracy',\n",
    "    save_best_only=True, save_weights_only=True, verbose=1)\n",
    "change_lw = models.LossWeightsModifier(lossweight = lossweight,\n",
    "                               initial_lw = initial_lw,\n",
    "                               directory = directory)\n",
    "lr_decay = keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc176a2",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48f8ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_save_dir = str(directory+'/trained_model.h5')\n",
    "try:\n",
    "    model.load_weights(model_save_dir)\n",
    "except:\n",
    "    history = model.fit(training_generator,\n",
    "                        steps_per_epoch = int(dataset['x_train'].shape[0] / train_params[\"batch_size\"]),\n",
    "                        epochs = train_params[\"n_epochs\"],\n",
    "                        validation_data = ([dataset['x_test'],\n",
    "                                            dataset['y_test_coarse'],dataset['y_test_medium'],dataset['y_test_fine']],\n",
    "                                           [dataset['y_test_coarse'],dataset['y_test_medium'],dataset['y_test_fine']]),\n",
    "                        callbacks = [tb,log,change_lw,lr_decay,checkpoint],\n",
    "                        verbose=1)\n",
    "    \n",
    "    model.save_weights(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361408c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    history_dict = history.history\n",
    "\n",
    "    plotter = tfdocs.plots.HistoryPlotter()\n",
    "    plotter.plot({\"Coarse\": history}, metric = \"prediction_coarse_accuracy\")\n",
    "    plotter.plot({\"Medium\": history}, metric = \"prediction_medium_accuracy\")\n",
    "    plotter.plot({\"Fine\": history}, metric = \"prediction_fine_accuracy\")\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.ylim([0,1])\n",
    "except:\n",
    "    print('Trained model weights loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ecc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plotter = tfdocs.plots.HistoryPlotter()\n",
    "    plotter.plot({\"loss\": history}, metric = \"loss\")\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.ylim([0,1])\n",
    "except:\n",
    "    print('Trained model weights loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08f53b",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3170fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = keras.Model(model.inputs[:1], model.output)\n",
    "\n",
    "lossfn = models.MarginLoss()\n",
    "final_model.compile(optimizer='adam', \n",
    "                    loss={'prediction_fine' : lossfn,\n",
    "                          'prediction_medium' : lossfn,\n",
    "                          'prediction_coarse' : lossfn},\n",
    "\n",
    "                    loss_weights={'prediction_fine' : lossweight['fine_lw'],\n",
    "                                  'prediction_medium' : lossweight['medium_lw'],\n",
    "                                  'prediction_coarse' : lossweight['coarse_lw']},\n",
    "\n",
    "                    metrics={'prediction_fine': 'accuracy',\n",
    "                             'prediction_medium': 'accuracy',\n",
    "                             'prediction_coarse': 'accuracy'\n",
    "                            }\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_analysis = models.model_analysis(final_model, dataset)\n",
    "results = model_analysis.evaluate()\n",
    "predictions = model_analysis.prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = [dataset['y_test_coarse'],dataset['y_test_medium'],dataset['y_test_fine']]\n",
    "pred_label = [predictions[0],predictions[1],predictions[2]]\n",
    "metrics.lvl_wise_metric(true_label,pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_measurements,consistency,exact_match = metrics.hmeasurements(true_label,\n",
    "                                       pred_label,\n",
    "                                       dataset['tree'])\n",
    "print('\\nHierarchical Precision =',h_measurements[0],\n",
    "      '\\nHierarchical Recall =', h_measurements[1],\n",
    "      '\\nHierarchical F1-Score =',h_measurements[2],\n",
    "      '\\nConsistency = ', consistency,\n",
    "      '\\nExact Match = ', exact_match,\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22240309",
   "metadata": {},
   "source": [
    "# Training Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d85e7c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_save_dir = str(directory+'/trained_model_2.h5')\n",
    "try:\n",
    "    model.load_weights(model_save_dir)\n",
    "except:\n",
    "    history = model.fit(training_generator,\n",
    "                        steps_per_epoch = int(dataset['x_train'].shape[0] / train_params[\"batch_size\"]),\n",
    "                        epochs = train_params[\"n_epochs\"],\n",
    "                        validation_data = ([dataset['x_test'],\n",
    "                                            dataset['y_test_coarse'],dataset['y_test_medium'],dataset['y_test_fine']],\n",
    "                                           [dataset['y_test_coarse'],dataset['y_test_medium'],dataset['y_test_fine']]),\n",
    "                        callbacks = [tb,log,change_lw,lr_decay,checkpoint],\n",
    "                        verbose=1)\n",
    "    \n",
    "    model.save_weights(model_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1408784",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9635f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter()\n",
    "plotter.plot({\"Coarse\": history}, metric = \"prediction_coarse_accuracy\")\n",
    "plotter.plot({\"Medium\": history}, metric = \"prediction_medium_accuracy\")\n",
    "plotter.plot({\"Fine\": history}, metric = \"prediction_fine_accuracy\")\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter()\n",
    "plotter.plot({\"loss\": history}, metric = \"loss\")\n",
    "plt.title(\"Model Loss\")\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43e938",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = keras.Model(model.inputs[:1], model.output)\n",
    "\n",
    "lossfn = models.MarginLoss()\n",
    "final_model.compile(optimizer='adam', \n",
    "                    loss={'prediction_fine' : lossfn,\n",
    "                          'prediction_medium' : lossfn,\n",
    "                          'prediction_coarse' : lossfn},\n",
    "\n",
    "                    loss_weights={'prediction_fine' : lossweight['fine_lw'],\n",
    "                                  'prediction_medium' : lossweight['medium_lw'],\n",
    "                                  'prediction_coarse' : lossweight['coarse_lw']},\n",
    "\n",
    "                    metrics={'prediction_fine': 'accuracy',\n",
    "                             'prediction_medium': 'accuracy',\n",
    "                             'prediction_coarse': 'accuracy'\n",
    "                            }\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595d4336",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_analysis = models.model_analysis(final_model, dataset)\n",
    "results = model_analysis.evaluate()\n",
    "predictions = model_analysis.prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70cbf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = [dataset['y_test_coarse'],dataset['y_test_medium'],dataset['y_test_fine']]\n",
    "pred_label = [predictions[0],predictions[1],predictions[2]]\n",
    "metrics.lvl_wise_metric(true_label,pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a439b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_measurements,consistency,exact_match = metrics.hmeasurements(true_label,\n",
    "                                       pred_label,\n",
    "                                       dataset['tree'])\n",
    "print('\\nHierarchical Precision =',h_measurements[0],\n",
    "      '\\nHierarchical Recall =', h_measurements[1],\n",
    "      '\\nHierarchical F1-Score =',h_measurements[2],\n",
    "      '\\nConsistency = ', consistency,\n",
    "      '\\nExact Match = ', exact_match,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d826e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
